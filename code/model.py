from langchain_ibm import ChatWatsonx
from langchain_core.prompts import PromptTemplate
from config import params, url, api_key, project_id, LLAMA_MODEL_ID, GRANITE_MODEL_ID, MISTRAL_MODEL_ID
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser

class AIResponse(BaseModel):
    summary: str = Field(description= "summary of the user's message")
    sentiment: int = Field(description=  "Sentiment score from 0 (negative) to 100 (positive)")
    response: str = Field(description = "Suggested response to the user")

json_parser = JsonOutputParser(pydantic_object = AIResponse)

def initialize_model(model_id):
    return ChatWatsonx(model_id= model_id, url= url, apikey=api_key, project_id=project_id, params= params)


llama_llm = initialize_model(LLAMA_MODEL_ID)
granite_llm = initialize_model(GRANITE_MODEL_ID)
mistral_llm = initialize_model(MISTRAL_MODEL_ID)

# Prompt template
llama_template = PromptTemplate(
    template='''<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>
{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
''',
    input_variables=["system_prompt", "user_prompt"]
)

granite_template = PromptTemplate(
    template="<|system|>{system_prompt}\n\<|user|>{user_prompt}\n<|assistant|>",
    input_variables=["system_prompt", "user_prompt"]
)

mistral_template = PromptTemplate(
    template="<s>[INST]{system_prompt}\n{user_prompt}[/INST]",
    input_variables=["system_prompt", "user_prompt"]
)

def get_ai_response(model, template, system_prompt, user_prompt):
    chain = template | model
    return chain.invoke({'system_prompt': system_prompt, 'user_prompt': user_prompt, 'format_prompt':json_parser.get_format_instructions()})

def llama_response(system_prompt, user_prompt):
    return get_ai_response(llama_llm, llama_template, system_prompt, user_prompt)

def granite_response(system_prompt, user_prompt):
    return get_ai_response(llama_llm, llama_template, system_prompt, user_prompt)

def mistral_response(system_prompt, user_prompt):
    return get_ai_response(llama_llm, llama_template, system_prompt, user_prompt)


